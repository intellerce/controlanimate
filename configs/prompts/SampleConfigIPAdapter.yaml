# Config File:

######################################################
# INPUTS
######################################################
input_video_path: "tmp/dance6_cropped2.mp4" # Path to the input video file
output_video_dir: "tmp/output"  # Directory to save the outputs

save_frames: 1 # 0: No, 1: Yes

# Width and Height of the Input and Output videos
# If zero the input's video dimension will be used otherwise the input will be resized
width: 512
height: 768

#(blurred background)+
prompt: "(Highest quality)++ colorful++ detailed++ beautiful++ smiling++ asian++ woman+ with (perfect pretty face)++ (female villain)+, (perfect big eyes)++ (muscle body)0.2, (fully dressed)++, (sharp features)+, (intense eyes)+, (commanding posture)+,, (phenomenal aesthetic)+, sumptuous artwork, breath of the wild, masterpiece, best quality, sharp focus, dramatic lighting, 8k, uhd, dslr, vivid color, cinematic lighting , matte painting , cinematic photo, realistic photo, body photo, walking forward, high quality"
n_prompt: "easynegative+, man++, boy++, pale++, (dark hands)+++, tint++, dark++, gloomy++, nudity, mask++, (bad face)+++, (bad mouth)+++, (worst quality)+, (low quality)+, lowres, bad anatomy, (monochrome)++, (grayscale)+, (text, font, logo, copyright, watermark)++, wrong face, wrong hands, wrong legs, wrong feet, (nsfw,nude)+"

# Additional Basic Parameters
start_time: "00:00:01" # Time in HH:MM:SS format to start reading the input video
end_time: "00:00:12" # Time in HH:MM:SS format to stop reading the input video

# Use the last frame of each AnimateDiff output sequence as reference for the next epoch using the following img2img strength
overlap_strength: .85

# Native LCM Model?
use_lcm: 0 # 0: No, 1: Yes - Use original LCM model. If used, the settings related to LoRA and DreamBooth will be ignored as LCM models do not support them.
# It worth noting that LCM does not have support for negative prompt at the moment.
# Also, it should be noted that LCM-LoRA is different from native LCM and its usage does not disable any other features.

use_img2img: 0 # 0: No, 1: Yes - Use img2img for non-overlapping frames. If no, then last output frame will be used as base for the added noise of non-overlapping frames.

######################################################
# MODELS
######################################################

# Base model that AnimateDiff uses to create the initial architecture from (it needs to be in HuggingFace format (.bin))
pretrained_model_path: "models/StableDiffusion/stable-diffusion-v1-5"

# Optional Alternative AutoEncoder (VAE)
vae_path: "models/VAE/vae-ft-mse-840000-ema-pruned.ckpt" #"models/VAE/vae-ft-ema-560000-ema-pruned.safetensors"

# Optional DreamBooth Model (full)
dreambooth_path: "models/DreamBooth_LoRA/absolutereality_v181.safetensors"  #"models/DreamBooth_LoRA/aZovyaRPGArtistTools_v3.safetensors" 

# Optional LoRA model to be used
lora_model_paths: 
  - "models/DreamBooth_LoRA/lcm_lora.safetensors" 
  # - "models/DreamBooth_LoRA/PsychedelicFluids.safetensors"
lora_weights: 
  - 0.8
  # - 0.8

# Motion Module to be used - versions of the config and the model must match
inference_config_path: "configs/inference/inference-v2.yaml"
motion_module: "models/Motion_Module/mm_sd_v15_v2.ckpt"

# ControlNets
# Optional ControlNet Models to be used - will be downloaded automatically
controlnets:
  - lllyasviel/control_v11p_sd15_openpose
  - lllyasviel/control_v11p_sd15_lineart
  - lllyasviel/control_v11p_sd15_mlsd
  - lllyasviel/sd-controlnet-canny
  - lllyasviel/control_v11p_sd15_softedge
  - lllyasviel/sd-controlnet-mlsd
  # - lllyasviel/control_v11p_sd15_normalbae
  # - lllyasviel/sd-controlnet-openpose
  # - lllyasviel/control_v11p_sd15s2_lineart_anime
  # - lllyasviel/sd-controlnet-hed

cond_scale:
  - 1.0
  - 0.3
  - 1.0
  - 0.2
  - 0.5
  - 1.0
  # - 0.45
  # - 1.0
  # - 0.75
  # - 0.05

guess_mode: 1 # 0: No, 1: Yes - To use guess mode in controlnet or not.

loop_back_frames: 1 # 0: No, 1: Yes - To use generated overlapping frames as inputs for the ControlNets or not.

######################################################
# PARAMETERS
######################################################


# IP-Adapter:
use_ipadapter: 1 # 0: No, 1: Yes
ipa_scale: 0.65 # Strength of IP-Adapter
do_initial_generation: 0 # 0: No, 1: Yes -> Generate a few initial frames to be used as baseline for the next image generations.


# Upscaling and Face Restoration:
upscale: 2 # Upscaler value for the input image
use_face_enhancer: 1 # 0: No, 1: Yes
upscale_first: 1 # Upscale before applying face enhancement - better results but slower: 0: No, 1: Yes

frame_count: 16 # How many co-related frames are produced by AnimateDiff - defaults to 16
overlap_length: 8 # Number of frames from previous output frames to be present in the current frames (helps with consistency)

seed: 23711 # Random Seed
steps: 12 # Denoising steps
guidance_scale: 1.35  
strength: 1.0 # Strengtht of the noise to be added to input latents - if 1.0 the img2img effect is nil


# Choice of scheduler: "DDIMScheduler", "EulerDiscreteScheduler" ,"DPMSolverMultistepScheduler","EulerAncestralDiscreteScheduler","LMSDiscreteScheduler","PNDMScheduler", "LCMScheduler"
scheduler: "LCMScheduler"  


fps: 15 # The framerate to sample the input video
fps_ffmpeg: 30 # The framerate of the output video (FFMPEG interpolation will be used if greater than fps)
crf: 23 # A measure of quality - lower is better 


######################################################
# ADDITIONAL
######################################################

ffmpeg_path: "/usr/bin/ffmpeg"