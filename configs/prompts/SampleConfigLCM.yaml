# Config File:

######################################################
# INPUTS
######################################################
input_video_path: "tmp/dance6_cropped2.mp4" # Path to the input video file
output_video_dir: "tmp/output"  # Directory to save the outputs

save_frames: 1 # 0: No, 1: Yes

# Width and Height of the Input and Output videos
# If zero the input's video dimension will be used otherwise the input will be resized
width: 512
height: 768

prompt: "A woman with perfect++ face++ (female villain)+, (perfect face)++, (perfect eyes)++ (bad face)----, (bad hands)----, (worst quality)---- (plain bright golden room)+++ (muscle body)0.2, fully dressed, (gold pants)++ , (gold suit)++ (gold shirt)+ (golden clothes)++, (sharp features)+, (intense eyes)+, (commanding posture)+, (aura of malevolence and mystery)+, (phenomenal aesthetic)+, sumptuous artwork, breath of the wild, masterpiece, best quality, sharp focus, dramatic lighting, 8k, uhd, dslr, vivid color, cinematic lighting , matte painting , cinematic photo, realistic photo, body photo, walking forward, high quality"
n_prompt: "easynegative+, nudity, tint, blue tint, blue, mask++, (bad face)+++, (worst quality)+, (low quality)+, lowres, bad anatomy, (monochrome)++, (grayscale)+, (text, font, logo, copyright, watermark)++, wrong face, wrong hands, wrong legs, wrong feet, (nsfw,nude)+"

# Additional Basic Parameters
start_time: "00:00:01" # Time in HH:MM:SS format to start reading the input video
end_time: "00:00:05" # Time in HH:MM:SS format to stop reading the input video

# Use the last frame of each AnimateDiff output sequence as reference for the next epoch using the following img2img strength
overlap_strength: 0.4



use_lcm: 1 # 0: No, 1: Yes - If used most of the settings related to LoRA, DreamBooth, ControlNet, will be ignored as LCM models do not support them.
# It is also worth noting that LCM does not have support for negative prompt at the moment.
strength: .5 # Strengtht of the noise to be added to input latents - if 1.0 the img2img effect is nil

use_img2img: 1 # 0: No, 1: Yes - Use img2img for non-overlapping frames. If no, then last output frame will be used as base for the added noise of non-overlapping frames.

######################################################
# MODELS
######################################################

# Base model that AnimateDiff uses to create the initial architecture from (it needs to be in HuggingFace format (.bin))
pretrained_model_path: "models/StableDiffusion/stable-diffusion-v1-5"

# Optional Alternative AutoEncoder (VAE)
vae_path: "models/VAE/vae-ft-mse-840000-ema-pruned.ckpt" #"models/VAE/vae-ft-ema-560000-ema-pruned.safetensors"

# Optional DreamBooth Model (full)
dreambooth_path: "models/DreamBooth_LoRA/dreamshaper_8.safetensors" 

# Optional LoRA model to be used
lora_model_paths: 
  # - "models/DreamBooth_LoRA/lcm_lora.safetensors" 
  # - "models/DreamBooth_LoRA/PsychedelicFluids.safetensors"
lora_weights: 
  # - 1.0
  # - 0.8

# Motion Module to be used - versions of the config and the model must match
inference_config_path: "configs/inference/inference-v1.yaml"
motion_module: "models/Motion_Module/mm_sd_v15.ckpt"

# LCM U-net Model Path:
pretrained_lcm_model_path: "models/LCM_Dreamshaper_v7"

# Optional ControlNet Models to be used - will be downloaded automatically
controlnets:
  # - lllyasviel/control_v11p_sd15_openpose
  # - lllyasviel/control_v11p_sd15_lineart
  # - lllyasviel/control_v11p_sd15_mlsd
  # - lllyasviel/sd-controlnet-canny
  # - lllyasviel/control_v11p_sd15_softedge
  # - lllyasviel/sd-controlnet-mlsd
  # - lllyasviel/sd-controlnet-openpose
  # - lllyasviel/control_v11p_sd15s2_lineart_anime
  # - lllyasviel/sd-controlnet-hed

cond_scale: 
  # - 0.1
  # - 0.35
  # - 1.0
  # - 0.4

guess_mode: 0 # 0: No, 1: Yes - To use guess mode in controlnet or not.

loop_back_frames: 1 # 0: No, 1: Yes - To use generated overlapping frames as inputs for the ControlNets or not.

######################################################
# OTHER PARAMETERS
######################################################

# IP-Adapter:
use_ipadapter: 0 # 0: No, 1: Yes
ipa_scale: 0.65 # Strength of IP-Adapter
do_initial_generation: 1 # 0: No, 1: Yes -> Generate a few initial frames to be used as baseline for the next image generations.




upscale: 4 # Upscaler value for the input image
use_face_enhancer: 1 # 0: No, 1: Yes
upscale_first: 1 # Upscale before applying face enhancement - better results but slower: 0: No, 1: Yes

frame_count: 16 # How many co-related frames are produced by AnimateDiff - defaults to 16
overlap_length: 8 # Number of frames from previous output frames to be present in the current frames (helps with consistency)

seed: 46700 
steps: 4
guidance_scale: 7.5



# Choice of scheduler: "DDIMScheduler", "EulerDiscreteScheduler" ,"DPMSolverMultistepScheduler","EulerAncestralDiscreteScheduler","LMSDiscreteScheduler","PNDMScheduler"
scheduler:  "LCMScheduler"


fps: 15 # The framerate to sample the input video
fps_ffmpeg: 30 # The framerate of the output video
crf: 23 # A measure of quality - lower is better 


######################################################
# ADDITIONAL
######################################################

ffmpeg_path: "/usr/bin/ffmpeg"